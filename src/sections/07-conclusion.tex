\section{Conclusion}
\label{sec:conclusion}

This master thesis has attempted to tackle the problem of machine-generated text detection through the lens of a marked attention to model efficiency and size.
The early chapters served to introduce the field and language modelling in general, hopefully helping to initiate those who were so far unfamiliar with the topic.
Chapter \ref{sec:threats} provided an overview of thread models relating to NLG technologies, such as their potential harmful uses in misinformation campaigns, phishing, or false research.
This was meant as justification for the development of detection technologies: it would be wishful thinking to assume that all actors -- regardless of background, competence, or intention -- will disclose their use of language generation, but alerting users that they're dealing with machine text remains critical.
Following up on the outlined threats, Chapter \ref{sec:approaches} outlined some of the most recent detection approaches present in the scientific literature.
These range from statistical strategies relying on classical representations such as word-frequency vectors and TF-IDF, to solutions employing the same large language models in the detection effort.
Aside from the lively research landscape surrounding the field, especially in recent years, another development that was important to highlight was the movement towards increasingly heavy detection systems.
LLMs and other large transformer-based approaches are responsible for the latest state-of-the-art performance, but these architectures also trade often minor gains for compute requirements that relegate the usage of these solutions to dedicated servers, precluding end users from locally executing software they rely on.

The principal objective of this master thesis is to contribute to the conversation surrounding detection systems by proposing alternative solutions that approximate SOTA performance while maintaining a lean model constitution.
The battlefield of choice was Task 8 at SemEval-2024 \citep{wang2024semeval}, a shared task built around black-box detection of machine generated text.
Model development targeting the shared task took place in roughly two stages: the first while SemEval-2024 was ongoing, resulting in official submissions to the leaderboards under the banner of team TueCICL, and a later phase in the context of this thesis.

The models submitted to the task spanned to subtasks, one consisting in binary classification (subtask A) and the other in change point detection (subtask C).
For both subtasks, the approach was to build an ensemble combining representations from a character-level mode, a TF-IDF model, and a third model build on linguistically motivated features.
This effort resulted in middle-of-the pack rankings, as the submitted solutions fell short of the baseline in both instances, and did not show the convincing performance they exhibited in development.

The second phase of development carried on the work on subtask A, a competition track that challenged participants in pure binary classification over human and machine-generated texts.
Having drawn valuable lesson from the experience during the shared task, and with a more firm grounding the latest research, a new batch of models was developed.
The major introduction in the new stage was to split up the general problem of machine-generated text detection into several sub-problems, aiming to determine whether a target text had been generated by some particular model.
Fifteen single-generator classifiers were trained, three for each of the five models that were included in the data provided for the shared task.
Of the three single-generator classifiers targeting each LLM, one was obtained by fine-tuning DistilBERT, and the other by fitting a random forest classifier with either TF-IDF vectors or linguistically motivated feature representations.
These models were then combined into an ensemble model, which processed each target text by computing the probability that it had been generated by each of the single-generator classifiers, then applying a feed-forward network on the obtained 15-feature vector representation.
This ensemble displayed performance above the task-winning model, and even an ensemble variant that did not have the transformer-based components achieved accuracy levels that would nearly have placed it on the podium.
These results strongly support the argument brought forward by this thesis, i.e., that there exist answers other than huge transformers along the development of high-performing systems.

Team Genaios \citep{sarvazyan-etal-2024-genaios} submitted the system they named LLMixtic, the solution that won the shared.
This model combined token-level probabilistic features extracted with LLaMA-2 models, placing it firmly in the LLM-based category.
Compared to this formulation, the model proposed in this thesis achieves more than comparable performance with only DistilBERT as a transformer-based module, a much smaller model family than Facebook's LLaMA.
Even without DistilBERT, model performance remains high, albeit lower than would be required for the shared task podium.
TF-IDF and especially linguistic features bring another advantage to the table: since the resulting representation is tied to surface-level, conceptually solid aspects, such as the frequency and rarity of a unigrams and bigrams, or some linguistic aspect such as POS-tag distributions, it is possible to trace the final prediction back to these characteristics of the input text.
Even though there weren't any explainability analyses carried out in this work, it is in theory possible to extend the simple binary classification with a natural language reason as to \emph{why} a particular decision was made.

Of course, the comparison with the task-winning model offered by team Genaios is not a very fair one, since test set labels were not available during the shared task.
The model proposed in this work was not developed in the dark, and the possibility to check performance on the test set was used multiple times to determine if an acceptable end result had been reached.
There is a popular saying in the digital space, that hindsight is 20-20, meaning literally that it has perfect eyesight in both eyes.
This phrase describes the follow-up experiments rather aptly: there was a wealth of experience around what had worked and what had not, and a better solution could be built on top of well-understood failures.
This of course only means that the comparison to team Genaios is not exactly correct -- in fact, their achievement, considering the difficulty of the shared task, is more impressive -- but does not take away from the final results that were observed.
On the other hand, the test set was of course not used in training in the post-deadline additions made in this work, making the presented solution at least equally as informative as those submitted during the shared task.

There are also other limitations to the approach outlined in this thesis, which should be mentioned.
One such aspect has to do with the composition of the test set.
Referring back to Table \ref{table:adata}, one can note that there are two aspects that make the test set perhaps not an accurate representation of the potential real-world detection landscape.
First, the only new model introduced by the test set is GTP-4, which has two of its predecessors in the train set (ChatGPT and Davinci-003).
In addition, the test partition contains documents sourced from only one domain: student essays from Outfox \citep{koike2024outfox}.
On the one hand, this puts into question exactly how well do proposed models perform in the black-box context, since it can be argued that the "surprise generator" introduced in the test set may not be very surprising.
On the other, while the Outfox data was not present in either partition other than the test set, it is still only one domain, which makes does not necessarily translate to other generation contexts.
Some domains in which detecting generation may be critical, but that are not present in this dataset, are e-mails (for example related to phishing and scamming attempts), product or place reviews, or general digital content generations, such as blogs or articles, which may be used in disinformation campaigns.

Another limitation of this work is the absence of additional tests concerning adversarial robustness.
Machine-generated text detection is a field particularly characterized by the race between detectors and evaders.
As detection mechanisms become more sophisticated, there comes a growing incentive to develop strategies to evade detection, for example to bypass AI checks that are being rolled out in academic software to ensure that students do not cheat.
Other research \citep{Crothers_2022} has found that statistical features, such as the language metrics included in this study, help the model resist adversarial attacks, since language features are less easily perturbed than LLM embeddings.
The ensemble model presented in this work would benefit from checks to its adversarial robustness, at least to verify if there's a benefit in this area from employing statistical features.

On this note, future research on this model should explore performance on other datasets, as well as in downstream application with real-world users, since it was designed with true deployment in mind.
Another avenue of experimentation could come from updating or adding modules to the ensemble.
Aside from TF-IDF, linguistic features, and DistilBERT, other useful information sources might come from other similar-sized, but differently tuned language models.
Instruction-tuned or chat-based models could provide valuable feedback in more interactive applications, and auxiliary systems augmented with information retrieval, capable of comparing the suspected generation against a corpus of available information, might take the degree of factuality of the target text in consideration when outputting a prediction.
A more competently set-up character-level model approach may also be more informative than the official results in subtask A might suggest -- after all, the ensemble components more closely associated with style, TF-IDF and language features, performed very well in the post-deadline experiments.
There may be more value to be extracted from the characters, especially since they are more commonly paired with convolutional networks in research, whereas they were used alongside an LSTM in this work, an architecture that is not as well-equipped to deal with the long sequence lengths that come with character-level representations.
In addition, the shared task itself contains two interesting directions that were not explored in relation to this ensemble model.

Subtask C of the shared task consisted in change point detection, a detection scenario in which a human author provides the first part of the text, which is then completed by an AI agent.
The use of NLG is harder to detect in this setup, thus making this strategy common in adversarial evasion techniques.
Drawing from the lessons in the simpler binary classification task, more work could be dedicated to change point detection.
Some of the linguistic features do not translate well, which is a challenge, since moving from a global to a per-token or per-subsequence measure is not equally easy for all representations.
Nonetheless, there's little doubt that future contributions to the field cannot afford to simplify their worldview to one in which a text is either fully generated, or fully not.

Alongside the monolingual, English-only dataset, subtask A had a multilingual variant as well.
This parallel subtask was barely mentioned in this work, since it was not undertaken in the official submissions, and the post-deadline additions failed to address it.
However, LLMs are not used only in English -- in fact, cross-linguistic and cross-task performance is one of the evaluation metrics for modern language models.
Smaller models, such as the one proposed in this work, may struggle as the definition of the problem they are solving grows wider -- but this is the exact problem ensembling smaller solutions was meant to address.
As the winning team for the multilingual track, team USTC-BUPT \citep{guo2024ustc} also use a bipartite strategy: after detecting the language of the text, they use a classification head over LLM embeddings for English, and a fine-tuned mT5 variant with a classification token for all other languages.
There is no reason why the specialists that are combined to form the ensemble in this work could not be extended to include classifiers for other languages.
The ways in which such an interplay of modules could be architected are extremely diverse and certainly entice a model designer's creativity.

While more work remains to be done in the field of machine-generated text detection, this master thesis has successfully shown that incremental adoption of LLMs and other similarly huge models is not the only frontier left to explore.
Solutions designed to run locally may be regarded as fringe solutions in the current landscape, but may become critically important in the future.
Research in this direction may not produce the highest-performing state-of-the-art systems, but it will certainly cover the many use cases in which detection software cannot be offered as a remote service.
Though different researchers and developers are bound to have different views in what they regard as interesting and innovative, there's also a point to be made in favor of the creativity inherent in making the most of whatever resources are available, as opposed to picking the latest off-the-shelf transformer model and fine-tuning it on the latest data collection.
Putting together a system that may be odd and wonky at the start and taking it off the ground offers a sense of achievement if it succeeds and a wealth of lessons when it fails, only to the benefit of end users.