\section{Task 8 at SemEval 2024}
\label{sec:task}

The 18th International Workshop on Semantic Evaluation (SemEval-2024) was a large event in NLP, offering various challenges that teams across the world could undertake.
Task 8 at SemEval-2024 \citep{wang2024semeval} centered around machine-generated text detection in a black-box setting (i.e., the generator models for the test set were not known while the competition was ongoing), in both monolingual and multilingual components.

This shared task spanned 3 subtasks: subtask A a binary classification task between generated and human texts, subtask B consisted in multi-class classification between multiple LLM generators, as well human texts, while subtask C was a boundary detection problem, where participants had to correctly pinpoint the boundary between a human and a machine-generated segment in each test.
During the active phase of SemEval-2024, which ended in February 2024, I undertook Task 8 jointly with Daniel Stuhlinger, a fellow student at the University of Tübingen.
The full report of our participation, carried out under the team name "TueCICL", can be viewed in \citet{stuhlinger-winkler-2024-tuecicl}.

As team TueCICL, we submitted results for two of the subtasks to the leaderboards, namely subtask A and subtask C, which were the two subtasks involving classification between solely human and machine-generated texts, whereas subtask B focused more on inter-generator differentiation.
For subtask A, consisting in binary classification over the whole texts, there were two phases development, one for shared task proper, and subsequent post-deadline experimenting in the context of this Master Thesis.
Due to the more pronounced research interest in subtask A, as well as to better present the higher volume of material associated with it, this section shall first discuss subtask C, then dive deeper into subtask A later in this Chapter.

\subsection{Change point detection}

Subtask C of the shared task addresses detection environments that are characterized by active adversarial agents performing the generation.
Specifically, detection technologies have been observed to be weak to techniques also found in authorship obfuscation \citep{macko2024authorship}, such as paraphrasing \citep{krishna2024paraphrasing} and noise-introduction \citep{wang2021adversarial}.
Change point detection, which is the titular requirement of subtask C, addresses another way in which the use of NLG technology might be obfuscated.
In the scenario targeted by the subtask, a human segment ranging from 0\% to 50\% of the text is concluded by a machine-generated component, making the text as a whole much harder to flag as a generation.

For all subtasks, the data is sourced from the M4 \citep{wang-etal-2024-m4} dataset.
Task organizers provided generations by GPT variants and the LLaMA series \citep{touvron2023llama}, but unfortunately not in high abundance: the training set contained around 5000 texts, and was accompanied by development set containing a little over 500 more.
Table \ref{fig:taskc_data} offers an overview of the data distribution across the sets.
Alongside the general scarcity of training and cross-validation data, one should also note that a relevant portion of the data is entirely machine-generated, meaning the change index is exactly 0.
Though this of course preserves the objective of being able to reliably detect fully generated texts, it does cut into the already low amount of true change examples available for training.

The evaluation metric by which the submitted solutions are measured is Mean Absolute Error (MAE), i.e. the average absolute difference in the predicted change-point index and the true value.
The baseline suggested by the organizers was based on Longformer \citep{beltagy2020longformerlongdocumenttransformer}, a fine-tuned RoBERTa checkpoint specialized in long texts.
This baseline achieved an impressive 3.5 MAE on the development set -- a tough standard -- and a (much lower) 21.54 MAE on the test set, though the latter was of course not known at development time.


\begin{table}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/subtaskc-data.png}
    \caption{
        Dataset breakdown for subtask C from Task 8 at SemEval-2024.
        The number in “()” is the number of examples purely generated by LLMs, i.e., human and machine boundary index=0.
        LLaMA-2-7B* and LLaMA-2-7B used different prompts. Bold data is used in shared task training development, and test.
    }
    \label{fig:taskc_data}
\end{table}

Following the example set by the baseline, the solutions we devised did not attempt to predict the boundary change index directly -- instead, we performed binary classification at the token level, essentially asking whether any given token belongs to the human or the machine-generated segment.
In accordance with the general mission of producing solutions that could run on at least midrange home computers and laptops, large-scale transformer-based approaches, such as the one provided as the baseline, were discarded altogether.

Instead, we presented an ensemble model sourcing its input from two bidirectional long short-term memory, or LSTM \citep{hochreiter1997long}, models.
The first LSTM operated at the character level and did not include any elements of pertaining.
Preprocessing the input texts for this character-level model involved lowercasing, and mapping numerals and punctuation to a \verb|<NUM>| and a \verb|<PUNCT>| special token respectively.
Similarly, whitespace characters of any type (e.g. space, tab, newline) were also mapped to the special token \verb|<WS>|.
Classification then proceeds as outlined on the character level, and the first occurrence of the positive label (indicating that the token belongs to the machine segment) is taken as the boundary change index.
Naturally, the character-level index is traced back to the word-level position, as required by the task setting, where the position of the word in which the first positively classified character is located is taken as the final label.

The second LSTM relied on pretrained Word2Vec \citep{mikolov2013efficientestimationwordrepresentations} embeddings sourced from the Wiki2Vec project \citep{yamada2020wikipedia2vec}. Static embeddings have fallen out of favor lately due to the emergence of contextual embeddings derived from LLMs, but they offer the significant advantage of not requiring firing up a large language model at inference time, a property that made them well-suited for our objectives in subtask C.
While this LSTM performs word-level classification, as opposed to the character-level nature of the first model, many of the preprocessing steps are maintained.
Casting texts to lowercase letters is all the more relevant since the Word2Vec mapping is case sensitive, and also maintained mapping numerals, punctuation and white-space to special tokens.

At least on paper, employing pretrained Word2Vec embeddings introduces an important amount of information into the model that would not be possible to obtain solely based on the available training data.
However, for the character-level LSTM, there are no such mitigating factors.
From very early on, it was clear from intermediate results that the character-level LSTM struggled significantly, since it had no pretrained semantic context for the input letter sequences.
To address this excessive burden placed on the scarce training data to both inform on how human language works, as well as how to distinguish it from generations, we enrich the dataset provided for subtask C with that of subtask A.
While deep discussion of the latter is reserved for the later sections of this chapter, it should suffice to say that training data for subtask A contains over 100.000 records, providing ample linguistic material, albeit not targeted to the specific objective of subtask C.

Records imported from the training set for subtask A had the target label set to either 0 (for fully machine-generated texts), or the length of the tokenized text (for fully human texts).
Since this operation carries the inevitable side effect of drowning out the far fewer examples where a change occurs mid-text (which are available in subtask C's data only), all models are trained for 5 epochs on the enriched data, and then exclusively on the original task data.
Aside from the aforementioned optimizations, hyperparameter tuning over the characteristics of the models yielded that, for both LSTM's, a hidden size of 512 and 2 layers resulted in the highest validation performance.

After training the two base approaches, an ensemble model was also constructed combining the representations of the two LSTM's.
This consisted in a non-recurrent feed-forward network (FFN) whose inputs were the concatenated representations at the word level.
We applied this FFN head to the representations at each word-level token, outputting a binary label, the same way as the two LSTM's.
For the character-level model, this meant averaging the representation at every character for any given word, before passing the representation to the ensemble classifier, along with the Word2Vec-based model's embeddings.
Utilizing a simple FFN in this case was deemed to be the most efficient solution, since the interaction between the various elements of the sequence is already captured in the sub-models.
This is also similar to how token-level classification is performed with transformers, with a feed-forward classification head applied at each token.
The best-performing hidden size for this model was found to be 256.
Table \ref{tab:c_models} offers a summary of the models developed for subtask C.

\begin{table}[ht]
    \centering
    \begin{tabular}{llll}
        \hline
        \textbf{Model}  & \textbf{Type}          & \textbf{Number of LSTM layers} & \textbf{Hidden size} \\
        \hline
        Character-level & Long short-term memory & 2                              & 512                  \\
        Word2vec        & Long short-term memory & 2                              & 512                  \\
        Joint model     & Feed-forward network   & -                              & 256                  \\
        \hline
        \vspace{0.1cm}
    \end{tabular}
    \caption{Summary of models for subtask C. \textbf{*} Number of layers. \textbf{**} Hidden size.}
    \label{tab:c_models}
\end{table}


\subsection{Generated text detection}
\label{subsec:subtask_a}

Subtask A of Task 8 tackles machine-generated text detection in the classical sense, in the way that was discussed in Chapter \ref{sec:approaches}.
It offered both a monolingual and a multilingual track, both with their respective leaderboards, but our team decided to only submit for the monolingual track.

As hinted at in the previous section, the training set provided for this subtask contained over 100.000 records, with 5000 more provided for validation, sourced from the M4 \citep{wang-etal-2024-m4} dataset.
Table \ref{table:adata} offers an overview of the data distribution across the three partitions.
All sets offer a balanced mix of generated and human texts, with the training set containing outputs from 4 different models: Davinci-003 (a GPT-3 variant), ChatGPT, Cohere \footnote{\url{https://cohere.com/}} and Dolly-v2 \citep{DatabricksBlog2023DollyV2}. The development set adds BLOOMz \citep{muennighoff2023crosslingualgeneralizationmultitaskfinetuning} to the collection, and the test set introduces GPT-4 on top.
\footnote{The make-up of the test set was not known during the shared task, it was only made available after the submissions deadline.}
The intent of the authors to enforce a black-box setting shines through in the way they built the partitions: the development set includes a model unknown to the train set, and the test set contains another model absent from the other splits.

Alongside the partitioned dataset, the task organizers also provided a RoBERTa-based transformer baseline.
The evaluation metric for this subtask was accuracy.
The transformer baseline achieves an accuracy of 72\% on the development set and 88.47\% on the test set.
\footnote{Baseline accuracy on the test set was only made public after the submissions deadline.}

\begin{table}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/subtaska-data.png}
    \caption{Subtasks A: Monolingual Binary Classification. Data statistics over Train/Dev/Test splits.}
    \label{table:adata}
\end{table}

It should be highlighted the the composition of the training set proved a unique, and at times unhelpful challenge in tackling the shared task.
Evaluating potential solutions based on their ability to detect solely BLOOMz generations could, in theory, be interpreted as an ability to generalize the behavior of some known LMs to unknown ones.
However, there is also the possibility that selecting based on this development set achieves a more superficial objective: detecting BLOOMz generations in particular.
This moving objective in the setup of the development set has plagued much of the development for this subtask, and will remain somewhat of a theme all the way through the final, post-deadline experiments.

With that premise, the proceedings at the time of the shared task evolved along similar lines as those described above for subtask C.
The overarching goal remained to develop a workable solution at a low computational cost.
For this purpose, an ensemble approach was devised for subtask A as well, with some modifications compared to the approach to subtask C.
Our intuition was that surface-level and stylistic features would be more effective than semantics in discriminating between human and machine-generated text. To build on this idea, we developed three approaches, which were joined at the end in the ensemble.

The first approach involved training a character-level LSTM. We expected the stylistic features of the texts to be good indicators of the generator, and working at the character level is known to capture this information well.
For example, character n-gram models have been used successfully in the field of authorship attribution, which relies heavily on style \citep{stamatatos2013robustness}.

Following generally the same steps as for the character model in subtask C, the input texts were first tokenized, then all tokens were mapped to their lowercase variants, and lastly numerals and punctuation were mapped to a \verb|<NUM>| and a \verb|<PUNCT>| special token respectively.
White-space elements (space, tab, newline) were also mapped to a special token \verb|<WS>|.
At this point, the tokenized and transformed inputs were fed through an LSTM, and the representation of the last token was used for prediction.
Notably different from subtask C was the model architecture, which was uni-directional in this case.
During development, this model was trained on both the classification objective, as well as a language modeling objective, for similar reasons as the dataset enrichment in subtask C.
Since this character-level approach did not benefit from any external data that would have latently contributed by means of pre-training, the language modeling objective was added to extract as much linguistic information as possible from the available dataset.

The second approach was constructed along the lines of the first in terms of technical setup, but dealt with words rather than characters.
Large transformer-based solutions benefit from vast amounts of pre-training, but at a heavy computational cost -- using pretrained embeddings as model inputs appeared to be a good compromise between heavy models and training from scratch, as had been the case with the character-level LSTM.
We used the pretrained Word2vec embeddings from the Wikipedia2Vec \citep{yamada2020wikipedia2vec} project to map texts to vectors, but maintained the other steps, such as mapping numerals, punctuation and white-space to special tokens.
As opposed to the Word2Vec model in subtask C, this variant also uses a uni-directional LSTM, though no language modeling task is performed in this case.

The third approach was not recurrent in its nature -- instead, we used the TextDescriptives pipeline \citep{Hansen_2023} through spaCy \citep{honnibal2020spacy} to obtain 66 linguistically motivated features to globally represent the text.
Such linguistic features have a long tradition in NLP, for example in the field of readability analysis (for example \citealp{vajjala-meurers-2012-improving}), and have been observed to be valid and cheap-to-compute representations in a variety of settings.
Since they are most well known for capturing the style of a text, rather than semantics, they appear to be very well suited for the present task.
Additionally, we computed the mean perplexity of the document using GPT-2 \citep{Radford2019} and added it to the feature vectors.
This follows the idea that the perplexity of a document assigned by a LLM should be higher for human written texts than for machine generated texts \citep{chaka2023detecting}.
Our third approach computed this global feature vector for the input text, then generated a prediction through a simple MLP.
The model consisted of 3 linear layers with Tanh activation functions in between and was trained for 2000 epochs with a learning rate of 0.0003.

An ensemble model was finally formulated, consisting in a feed-forward network which takes as input the final representations (the last hidden states in the case of the LSTM's) of each of the three previous approaches, generating a single prediction.
In addition to the standard model weights and biases related to the linear layers, a trainable scalar weight was also specified for each of the three inputs, to allow for dynamic scaling of the three input vectors, which would have been otherwise hard to interpret in tandem.
Table \ref{tab:a_models} offers an overview of the models developed by team TueCICL during the shared task proper.

\begin{table}[ht]
    \centering
    \begin{tabular}{llll}
        \hline
        \textbf{Model}    & \textbf{Type}          & \textbf{Number of LSTM layers} & \textbf{Hidden size} \\
        \hline
        Character-level   & Long short-term memory & 2                              & 512                  \\
        Word2vec          & Long short-term memory & 2                              & 512                  \\
        Language features & Long short-term memory & 3                              & 256                  \\
        Joint model       & Feed-forward network   & -                              & 512                  \\
        \hline
        \vspace{0.1cm}
    \end{tabular}
    \caption{Summary of models for subtask A.}
    \label{tab:a_models}
\end{table}

Results for this and all other experiments will be discussed in detail in Chapter \ref{sec:discussion}, but it makes sense to give a few notes before moving on to the post-deadline contributions.
The shared task only allowed one submission per subtask, and out best-performing solution on the development set was the ensemble model, with an accuracy of 85\%, beating the baseline (at 72\%) by a substantial margin.
Despite this, the ensemble model only achieved an accuracy of 54.57\% accuracy on the test set, far behind the baseline's 88.47\%.
This observation remains true even at the level of the three sub-models, neither of which performs convincingly on the test set.
Surprisingly, evaluations made after the deadline, once the test set labels were published, revealed that our best submission came from a model that was never intended to be the front-runner in our research, namely a TF-IDF logistic regression approach that only reached 60\% accuracy on the development set, but achieved an impressive 87\% accuracy on the test set, and was thus our only model that managed to beat the baseline.

After looking at our various models' performance on the test set, it strongly appeared that solid performance on the development set did not translate into a good grasp of the test set.
Instead, our solutions had become presumably more optimized at detecting BLOOMz generations, as opposed to machine-generated text in general.
The best performing model on the test set ended up being one that did not show almost any promise in development.

In summary, two important lessons were drawn from the shared task results that informed the subsequent work on this subtask, which will be presented in detail in the next subsection.
First, the models did not achieve a high enough degree of sophistication, and failed to capture the characteristics of generated texts -- new solutions needed to be more firmly grounded in the current research around the best and most efficient detection systems.
Second, good performance on the provided development set did not indicate a good system overall.
Finding a more suitable evaluation method for the models in subsequent training iterations was essential to improve performance, otherwise the cross-validated definition of good detector and a good \emph{BLOOMz} detector would remain indistinguishable.

\subsection{Generated text detection: post-deadline additions}

Ensemble solutions to the problem of machine-generated text detection are far from rare in the scientific literature on the subject.
For example, \citet{frohling2021feature} propose ensemble approaches over linguistic features, TF-IDF models, and neural classifiers.
This could be seen as a more refined implementation of a similar intuition to what was presented in the previous section for subtask A, with two notable departures from that workflow: pretrained static embeddings like Word2Vec are not used, and the inputs to the ensemble are not  long vector representations, but individual model probabilities associated with the positive label.
Results obtained by \citet{frohling2021feature} in the multi-generator setting (the same situation as subtask A, where the data splits contain generations from multiple models) suggested that their detector might be developing specialized sub-detectors for each individual data source, instead of forming a generalized comprehension of the distinction between human-written text and machine outputs.
This hypothesis is echoed by other studies as well, which observe a generally low transferability of a detector's ability to detect \emph{some} model's to generations to detecting generated text in general.
For example, \citet{mitchell2023detectgpt} also observe a steep drop in detection performance in the black-box setting compared to the white-box setting, suggesting that ensemble solutions targeting different but \emph{particular} models might constitute a promising way forward in the field.

The work put forward in the following pages of this Master Thesis is developed along the guidelines mentioned in these recent research approaches.
These additions are made in the context of this thesis, and are distinct from the shared task at SemEval-2024 and do not steam from a team effort like TueCICL's submissions in the main event, discussed in the previous section.

A first direction for the post-deadline work was provided by the realization that the models developed for the shared task did not fully utilize the information provided in training and validation.
In fact, aside from the text, the domain (Wikipedia, Wikihow, et cetera) and the label (machine or human) of every record, the exact generator model was also known for machine-generated text.
Jointly with the evidence that detectors are much better suited to detect \emph{particular} models, as opposed to generated text in general, it was determined that the contributors to the final ensemble model in this second stage would be specialized models, trained to detect only one particular generator.

Of the original strategies devised for the shared task, the Word2Vec and the character-level approaches were discarded, since they underperformed their counterparts in the shared task submissions, which is a confirmation of their relatively lower representation in the literature.
Despite half of the previous approaches being discarded in this stage, it still makes sense to continue working with linguistic features.
They appear to give valuable contributions to detectors in the relevant research \citep{frohling2021feature}, and they were perhaps not sufficiently explored for the shared task submissions.
With linguistic features, it is possible to hand-pick a limited selection to ensure explainability of the results, but for pure performance it is generally preferred to aim for a higher quantity of metrics.
For the shared task, we computed the feature vectors through the TextDescriptives pipeline \citep{Hansen_2023} for spaCy, which yield about 60 distinct measures per text.
In the second stage, we swapped the extraction library to LFTK \citep{lee2023lftkhandcraftedfeaturescomputational}, a more recent and mature feature collection and framework, and also the largest off-the-shelf solution at the time of writing.

In addition, since the best-performing model for the original task unexpectedly ended up being TF-IDF, it seems fair to conclude that the information captured by these document matrices can be fruitfully applied to the tasked at hand.
\citet{frohling2021feature} also employ TF-IDF to successfully boost detection performance.

Lastly, large-scale transformer-based solutions such as RoBERTa are still off the table, but excluding all transformers altogether was perhaps overly hasty.
DistilBERT \citep{sanh2020distilbertdistilledversionbert} is a BERT version that retains much of its larger siblings' language capability at a lower computational cost.
Since inference with DistilBERT is still manageable with the target end-user systems, it is included as an ensemble component in this work, with the aim to obtain some of the predictive power of state-of-the-art transformer approaches.

This work breaks down the problem of black-box machine-generated text detection into a set of smaller problems, consisting in detecting the output of one generator at a time.
A number of classifiers are trained for each model contained in the train set: Davinci, ChatGPT, Dolly, and Cohere.
Each individual classifier is trained with one of three general strategies and is only optimized to detect one model.
To summarize the model strategies outline above, a detector model is trained for each generator model, sourcing its inputs either from linguistic features computed through LFTK, TF-IDF vectors, or DistilBERT.

To achieve the training objective for each sub-classifier, a secondary development is derived from the training set, containing 3000 records for each generator, and an extra 12.000 human texts, which yields balanced classes.
This is done both to set up a validation set that allows the best models to be selected only based on their ability to detect their own generations, as well as to maintain consistent train and development data for the sub-classifiers, in order to maintain a validation data split whose is unknown to all classifiers, which would not have been the case had a custom validation set been sampled individually for each classifier. Random state for every step is fixed at the same arbitrary constant value, and there was no supervised search performed for any step described below.

To train the TF-IDF and language feature-based classifiers, the process runs in a similar fashion.
First, all documents produced by the target generator are extracted from the train set, yielding around 10.000 documents.
The same amount of human records is sampled from the train set.
This means that in the training process for each classifier, human texts are significantly undersampled compared to all of the available data.
This is done to ensure balance between classes, and since the sampling is random for each classifier, it is statistically likely that all of the human texts will have been used by the time all classifiers are trained.
For TF-IDF, the documents are vectorized with lowercasing, stopword removal, and the top 100.000 most frequent terms are kept in the unigram-bigram range.
For the language features, the pre-computed feature vectors are taken from a database for time efficiency, but computing them at inference time in downstream applications would not be a technical hurdle.
A Random Forest Classifier is then fit to the input vectors, with standard hyperparameter tuning through grid search to evaluate a variety of configuration constellations.
Cross-validation for the classifier is not carried out through ten-fold cross-validation, instead the custom validation set (3000 human texts and 3000 documents generated by the target classifier) is supplied during training to obtain the best model.

For the DistilBERT fine-tuned classifier, the data partitions used in training are the same as for the statistical models.
The difference is of course in the model architecture, as well as the preprocessing, which is done by the associated DistilBERT tokenizer.
The training proceeds for 5 epochs, with the best model selected based on accuracy on the custom validation set.

Each individual classifier is then evaluated on all three non-train data partitions: the custom development set containing 24.000 records originally in the train set, the \emph{true} validation set with 5000 (2500 BLOOMz + 2500 human) generations, and the test set.
Table \ref{tab:subsolutions-initial} reports the accuracy achieved by the individual classifiers against the data partitions available at this stage.
Deeper discussion about the results can be found in Chapter \ref{sec:discussion}, but the results are included here for an intermediate sanity check.

\begin{table}[ht]
    \centering
    \vspace{0.1cm}
    \begin{tabular}{lp{5px}cccp{5px}cccp{5px}ccc}
        \hline
        \multirow{2}{*}{Model} &  & \multicolumn{3}{c}{Dev Set} &             & \multicolumn{3}{c}{True Dev Set} &  & \multicolumn{3}{c}{Test Set}                                                                            \\
                               &  & \tiny{TF-IDF}               & \tiny{Lang} & \tiny{BERT}                      &  & \tiny{TF-IDF}                & \tiny{Lang} & \tiny{BERT} &  & \tiny{TF-IDF} & \tiny{Lang} & \tiny{BERT} \\
        \hline
        ChatGPT                &  & 0.74                        & 0.86        & 0.88                             &  & 0.58                         & 0.50        & 0.60        &  & 0.86          & 0.82        & 0.83        \\
        Davinci                &  & 0.79                        & 0.95        & 0.88                             &  & 0.65                         & 0.59        & 0.63        &  & 0.89          & 0.76        & 0.81        \\
        Cohere                 &  & 0.71                        & 0.66        & 0.87                             &  & 0.66                         & 0.57        & 0.60        &  & 0.55          & 0.56        & 0.57        \\
        Dolly                  &  & 0.72                        & 0.85        & 0.89                             &  & 0.64                         & 0.54        & 0.69        &  & 0.76          & 0.42        & 0.64        \\
        \hline
        \vspace{0.1cm}
    \end{tabular}
    \caption{
        Classification accuracy by the individual classifiers on the various data partitions.
        The "Dev Set" is a split-off segment of the train set, the "True Dev Set" is the initial development set provided by the task organizers.
        "Lang" refers to the classifiers trained on linguistic features.
    }
    \label{tab:subsolutions-initial}
\end{table}

There are a few interesting observations that can be made in relationship with the classifiers' performance.
One aspect that was hard to miss among the test set results is that the TF-IDF classifier trained to detect the Davinci generator is the best solution ever to emerge for the shared task, beating all potential submissions -- though this of course it is only possible to say this after the fact, with access to the test set labels.
Without these, there is nothing in the runs on the validation set that would make this solution stand out.
Accuracy on the BLOOMz development set remained consistently low for all classifiers, but the worst classification performance among all was the features-based classifier trained for Dolly on the test set, which dipped below random chance.
Overall, the consistently higher results on the test set compared to the true development set reinforce the idea that trying to optimize for the original development set might be a fool's errand.
The results in this case clearly reflect the much higher similarity of the train set to the test set.
Generally, the feature and TF-IDF based models did not underperform DistilBERT on either set, which might have perhaps been the expectation, and TF-IDF even outperforms DistilBERT by a significant margin for the Dolly detectors, and generally do better for three out of four models on the test set.
This inspires some initial confidence in the new approach.

After obtaining the model classifiers, an ensemble model is trained sourcing the inputs from them.
The data used for training the ensemble is the custom development set mentioned earlier, containing 3000 examples from each generator and 12.000 human texts.
This is setup helps ensure that individual generator detectors haven't already seen the training example.
In truth, training the ensemble on the full train set or on this split-off segment wasn't observed to result in a different outcome in terms of performance on the original validation set, which was used to select the best model in this stage.
Following \citet{frohling2021feature}, a probability is inferred with each classifier for each target document, resulting in a 12-length input vector, with each value referring to the probability associated to the positive label according to one classifier.
Two model architectures are explored for the ensemble: a random forest classifier, trained with grid search in a similar fashion as the TF-IDF and language feature models, and a neural feed-forward network.
Table \ref{tab:ensemble-initial} presents the resulting ensemble classifiers.

\begin{table}[ht]
    \vspace{0.1cm}
    \centering
    \begin{tabular}{lp{150px}cc}
        \hline
        Model       & Description                                                               & Dev accuracy & Test accuracy \\
        \hline
        Statistical & Random Forest with Grid Search                                            & 0.89         & 0.82          \\
        FFN         & Neural feed-forward network, with 3 linear layers and a hidden size of 64 & 0.65         & 0.79          \\
        \hline
        \vspace{0.1cm}
    \end{tabular}
    \caption{Accuracy of the ensemble models on the task development set (BLOOMz generator only) and the test set.}
    \label{tab:ensemble-initial}
\end{table}

The final ensemble classifiers do not represent a bad result in and of themselves -- in fact, submitting either for the shared task would be constitute a 30\% improvement in accuracy on the official submissions, and would have placed better in the final leaderboard.
However, there is also reason to be disappointed.
Firstly, neither ensemble performs better than the best individual classifier on the test set, where the Davinci-optimized TF-IDF classifier achieved 89\%.
In addition, this second stage of experimentation also failed to produce a detector better or equalling the 90\% mark in classification performance.
There is nothing inherently special about this threshold, other than that it is about the minimum value that beats the baseline, but literature analysis and earlier results would have suggested that reaching is not only possible, but, at least in theory, likely.
Another important observation is that the best result ever observed on the development set, 89\% by the random forest ensemble, did not translate to a convincing result on the test set, again indicating that the development set acts more as an obstacle to overcome, rather than a true indication of performance, at least in the context of the shared task.

As a final round of experiments to check whether swapping the development set would indeed improve performance on the shared task, all steps described above are done anew, with a few modifications to the data partitions.
The original train and development sets, totalling a little more than 122000 documents combined, are merged, and a new development set is sampled from this combined partition.
After this process, the new development set contains 13000 human texts, 3000 text from each of Cohere, ChatGPT, Davinci, Dolly, and finally 1000 BLOOMz generations.
The new train set contains all remaining documents, i.e. 52851 human texts, 11343 from Davinci, 11339 from ChatGPT, 11046 from Dolly, 10.678 from Cohere, and finally 1500 from BLOOMz.

All single-generator classifiers were re-trained on these new data partitions.
For most classifiers, this did not introduce any change, since they were still only trained and evaluated on a single generator, therefore, for example, the ChatGPT classifiers were still only seeing human and ChatGPT texts.
A complete novelty compared to the earlier models are the three BLOOMz-based classifiers.
These models do not have nearly as much to train and evaluate on as their siblings, but their contribute may still reveal itself to be valuable.
It's worth mentioning that, just like in the earlier iteration of this experiment, the single-generator models' ability to detect their target model's generation is near-perfect across the board.

After obtaining the single-generator detectors, a set of new ensemble models are trained.
The ensemble inputs remain the probability of the positive label according to each single-generator classifier, since this was unlikely to be limiting factor in the earlier run.
This second run of experiments yielded much more convincing results, which will be explored in-depth in Chapter \ref{sec:discussion}.