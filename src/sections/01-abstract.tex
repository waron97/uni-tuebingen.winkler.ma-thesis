\section*{Abstract}

Recent developments in Natural Language Processing (NLP) have resulted in the development and popularization of highly effective Large Language Models (LLMs), capable of generating convincing and seemingly creative linguistic material.
As these technologies have gained more and more momentum and usage, so has the need for accurate detection strategies risen alongside them.
More sophisticated generators demand mature and creative detectors, and so a double arms race has formed: the systems designed to identify unauthentic texts find themselves tackling both an evolving landscape of text generation and a bustling environment of adversarial agents developing strategies to evade detection.
On the hunt for better and better models, the field of machine-generated text detection has tended towards employing those same large language models in its solutions, at the cost of high computational complexity and hardware requirements.
The design philosophy behind most of these systems seems to have accepted that end-users will not be able to own the detection software, as they will not be able to run it on their own machines.
This work attempts to bring this requirement back into focus and shows that it is possible to develop competitive solutions without invoking multi-billion parameter models, and does so by leveraging ensemble approaches and simple representation strategies such as TF-IDF and linguistically motivated features.
The final model presented in this thesis achieves top-of-the-rank performance on Task 8 at SemEval-2024, while only sparingly relying on transformer fine-tuning.