\section*{Abstract}

Recent developments in Natural Language Processing (NLP) have resulted in the development and popularization of highly effective Large Language Models (LLMs), capable of generating convincing and seemingly creative linguistic material.
LLMs have garnered much attention, both from researchers and the general public, and continue to be increasingly applied to a variety of fields. However, the breakneck speed at which these systems are adopted leaves unattended some of the security concerns regarding their use.
Since the language produced by the models is of such high quality, it is not always feasible to distinguish authentically human contributions from machine-generated text, which can enable a multitude of nefarious applications of LLMs.
This work explores the history and inner workings of LLMs, how they can be misused, and possible antidotes to the problem of machine-generated text detection, with a careful eye toward a good balance of computational cost and performance of detection strategies.
