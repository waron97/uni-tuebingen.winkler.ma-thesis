\section{Discussion of SemEval results}
\label{sec:discussion}

\subsection{Subtask A: Shared Task analysis rankings}

\begin{table}[ht]
    \centering
    \begin{tabular}{llll}
        \toprule
        \textbf{Model}     & \textbf{Development set} & \textbf{Test set} & \textbf{Ranking} \\
        \midrule
        Baseline           & 0.72                     & 0.88              & 20               \\
        \midrule
        Character-level    & 0.85                     & 0.55              & 127              \\
        Word2vec*          & 0.82                     & 0.72              & 85               \\
        Language features* & 0.63                     & 0.88              & 21               \\
        Joint model*       & 0.83                     & 0.69              & 96               \\
        \bottomrule
        \vspace{0.1cm}
    \end{tabular}
    \caption{Results for SemEval-2024 Task 8, subtask A. Dev and Test columns report the accuracy on the respective data partitions. The ranking column refers to the model ranking in the shared task competition. The scores and ranking of the unofficial submissions were not provided by the organizers and computed by team TueCICL. There was a total of 137 submissions.\\ \textbf{*} unofficial submissions}
    \label{tab:a_results}
\end{table}

Table \ref{tab:a_results} shows the results for each model submitted during the proceedings of the shared task, for subtask A.
On the development set, almost all models outperform the transformer baseline provided by the organizers.
The best performing model was the character-level model, with an accuracy of 0.85 -- this was our final submission for the shared task.
While the two recurrent models and the joint model do not differ very much from one another, the FFN built on linguistically motivated global feature vectors sets itself apart in that it is the worst performing model on the development set.

Perhaps the most important lesson here is that there's clearly diminishing returns in closely fitting the development set -- perhaps even negative returns, as the worst-performing model in development is the best-performing one on the test set by a big margin.
At any rate, it wouldn't be honest to put the blame of this middling performance on the development set alone.
Selecting character-level and Word2Vec-based solutions appears after the fact to have been a misfire as well.
For further information, the original task report \citep{stuhlinger-winkler-2024-tuecicl} contains an in-depth rundown of the shared task, while the following pages are dedicated to how these lessons can help improve performance on the task, with little adjustments to the methods, that nonetheless preserve the mission of computationally efficient, locally runnable detectors.

\subsection{Subtask A: post-deadline improvements}

Having taken note of the disappointing performance of the official submissions to the shared task, a new batch of experiments was conducted in the context of this Master Thesis.
These had the objective of being more firmly grounded in research, while still maintaining the objective of developing detectors which end-users would be able to own and run on their own machines.
A new approach was developed, targeting generator models one at a time with different strategies, since machine-generated detection seems to be an elusive target when targeting all generators at once.
Instead, the work was split among many different classifiers, attempting to differentiate between human texts and only one generator (Davinci, ChatGPT, Cohere, Dolly, BLOOMz).

The first batch of experiments resulted in the single-generator classifiers described in Table \ref{tab:subsolutions-initial}.
Among these models was the one showing the best ever performance on the test set, with an 89\% classification accuracy by the TF-IDF model on Davinci, though this model would have been impossible to find without access to the test set labels, since its performance on the development does not stand out.
After running the full experiment, which ended in the development of the ensemble model, it was also observed that the approach resulted in good, but not excellent classification performance.
Table \ref{tab:ensemble-initial} describes the two ensemble models, one neural and one trained with a random forest base.
Unfortunately, neither ensemble beats the baseline, and both fall short of the best single-generator classifier on the test set.
They also both underperform the best unofficial submission to the shared task, which was developed by team TueCICL, achieving 88\% test set accuracy with only linguistically motivated features.

Chapter \ref{sec:task} concluded with the description of the last experimental run, which followed the general lines of the first, with a re-sampled development set.
The single-generator model lineup was also extended to include the three BLOOMz classifiers, one with a fine-tuned DistilBERT model, and two statistical approaches with TF-IDF and language feature representations respectively.
The remainder of this section is dedicated to the analysis of this last experimental run, which resulted in a final ensemble with a classification accuracy of 97\% on the test set.

The first task in the newly formulated approach to the task of machine-generated text detection is obtaining a set of single-generator classifiers.
As a reminder, a total of 15 such classifiers, one for each model and strategy combination, are trained on a subset of the train data, containing only human texts and generations from the target model.
The validation set for these models is constructed along the same lines, containing no generations from other models, to ensure each single-generator classifier is selected based solely on its ability to detect its target.
Probability scores associated to the positive label are then used as input to the final ensemble classifier.


\begin{table}[ht]
    \centering
    \vspace{0.1cm}
    \begin{tabular}{llp{10px}ccp{10px}c}
        \toprule
        \multirow{2}{*}{Model} & \multirow{2}{*}{Strategy} &  & \multicolumn{2}{c}{Development set} &               & Test set                   \\
                               &                           &  & \tiny{Preicision}                   & \tiny{Recall} &          & \tiny{Accuracy} \\
        \midrule
        ChatGPT                & DistilBERT                &  & 1.00                                & 0.66          &          & 0.83            \\
        ChatGPT                & TF-IDF                    &  & 0.97                                & 0.55          &          & 0.88            \\
        ChatGPT                & Language Feats            &  & 0.98                                & 0.69          &          & 0.83            \\
        Davinci                & DistilBERT                &  & 0.95                                & 0.82          &          & 0.77            \\
        Davinci                & TF-IDF                    &  & 0.94                                & 0.68          &          & 0.89            \\
        Davinci                & Language Feats            &  & 0.97                                & 0.87          &          & 0.97            \\
        Cohere                 & DistilBERT                &  & 0.96                                & 0.82          &          & 0.69            \\
        Cohere                 & TF-IDF                    &  & 0.94                                & 0.66          &          & 0.70            \\
        Cohere                 & Language Feats            &  & 0.96                                & 0.65          &          & 0.45            \\
        Dolly                  & DistilBERT                &  & 0.87                                & 0.92          &          & 0.62            \\
        Dolly                  & TF-IDF                    &  & 0.89                                & 0.81          &          & 0.87            \\
        Dolly                  & Language Feats            &  & 0.99                                & 0.30          &          & 0.56            \\
        BLOOMz                 & DistilBERT                &  & 0.92                                & 0.10          &          & 0.55            \\
        BLOOMz                 & TF-IDF                    &  & 0.80                                & 0.32          &          & 0.54            \\
        BLOOMz                 & Language Feats            &  & 0.70                                & 0.57          &          & 0.56            \\
        \bottomrule
        \vspace{0.1cm}
    \end{tabular}
    \caption{
        Performance metrics for final single-generator classifiers.
        The development set referenced here is the one derived from the train set and described in Chapter \ref{sec:task}, not the original development set.
        Precision and recall refer to the \textbf{positive} label, not to the average of the metric over the two classes.
    }
    \label{tab:subsolutions-final}
\end{table}

The objective for single-generator classifiers is to be as specialized as possible in detecting generations from a single model.
In other words, they should be geared towards high precision, rather than high recall.
Table \ref{tab:subsolutions-final} provides a summary of the single-generator classifiers, with precision and recall for the positive label (i.e. for the machine-generated class) over the re-sampled development set, and accuracy on the test set.
There are several takeaways from this table that are worth mentioning.
Initially, it can be noted that precision is high for nearly all classifiers, which is in concordance with the models' training goals.
For some single-generator classifiers, even the recall value for the positive label is respectable, meaning that the models display good ability to detect generations from other models as well.
Another interesting aspect is that for all generators, at least one strategy displays high precision, even when other strategies struggle.
For example, BLOOMz-targeted models struggle when using TF-IDF and language features, but are rescued by their DistilBERT sibling.
Similarly, detectors for Dolly in the TF-IDF and DistilBERT strategies do not inspire high confidence, but the features-based classifier appears to have specialized much more than the others, and could come to the rescue for difficult scenarios despite its low test-set performance.
Feature-based detectors generally perform above expectations, with all except BLOOMz developing a highly specialized toolkit, resulting in especially high precision.
In this sense, they are perhaps the strategy that best captures the objective of the single-generator models: highly specialized systems, that can detect one specific model with very high precision.
As will be seen later, this is likely why they are a crucial driver for performance in the ensemble classifiers.

When it comes to performance on the test set, the results of course do not differ from those already reported in Table \ref{tab:subsolutions-initial}, since these are the same models, evaluated on the same data.
For the 12 single-generator classifiers that were already discussed, only the precision and recall metrics were new introductions to the conversation.
In that sense, the metrics paint a positive picture, with a good degree of specialization among models, with high precision being an indicator of success even in the face of low test-set accuracy.
The BLOOMz-targeted models were trained with only 1500 positive examples, with a further 1000 reserved for validation.
Predictably, this led to the non-pretrained models being at a disadvantage, since TF-IDF and feature-based models need more data than fine-tuning to achieve a good fit.
Indeed, the DistilBERT-based detector is the best-performing classifier in terms of precision, with the two other models lagging behind.
The scarcity of data appears to have hurt the features-based classifier the most, even though this strategy is very high-performing when detecting other generators.
Not only is the fine-tuned classifier the most precise, but it also appears to be the most highly specialized.
It achieved a recall of only 10\%, which one might mistakenly consider to be a negative measurement.
BLOOMz texts were only responsible for 3\% of generations in the re-sampled development set, meaning that the low recall measured for this model is a positive sign of highly optimized detection behavior, which again is the goal of the single-generator classifiers.
The TF-IDF and feature-based classifiers for BLOOMz do not display similar behavior, as their lower precision and higher recall indicate that they take more shots in the dark, which constitutes and undesirable, if perhaps inevitable outcome.
If any conclusion is to be drawn from the results in Table \ref{tab:subsolutions-final}, it would be that the process has seemingly produced at least one highly specialized detector for all generators, with some less precise but hopefully good auxiliary models.
It remains to be seen whether it is possible to obtain an ensemble that properly leverages the properties of the single-generator models.

Aside from verifying the models' performance on their own targets, it should also help to check in more detail how well these classifiers generalize to unknown generators.
Table \ref{tab:generalization} presents generalization metrics for the same models as above.
Each value in the table represents the recall score, i.e., the proportion of documents from a particular generator that a model correctly flagged as machine-generated, with detector-generator model concordance being bolded.

\begin{table}[ht]
    \vspace{0.1cm}
    \centering
    \begin{tabular}{llccccc}
        \toprule
        Model   & Strategy           & ChatGPT       & Davinci       & Cohere        & Dolly         & BLOOMz        \\
        \midrule
        ChatGPT & DistilBERT         & \textbf{1.00} & 0.74          & 0.68          & 0.39          & 0.15          \\
        ChatGPT & TF-IDF             & \textbf{1.00} & 0.64          & 0.43          & 0.25          & 0.13          \\
        ChatGPT & Language Features  & \textbf{1.00} & 0.60          & 0.50          & 0.86          & 0.03          \\
        Davinci & DistilBERT         & 0.99          & \textbf{0.99} & 0.78          & 0.63          & 0.47          \\
        Davinci & TF-IDF             & 0.85          & \textbf{1.00} & 0.60          & 0.40          & 0.27          \\
        Davinci & Language Features  & 0.94          & \textbf{1.00} & 0.87          & 0.90          & 0.23          \\
        Cohere  & DistilBERT         & 0.96          & 0.80          & \textbf{1.00} & 0.68          & 0.27          \\
        Cohere  & TF-IDF             & 0.66          & 0.59          & \textbf{1.00} & 0.51          & 0.35          \\
        Cohere  & Language Features  & 0.71          & 0.69          & \textbf{1.00} & 0.35          & 0.16          \\
        Dolly   & DistilBERT         & 0.98          & 0.86          & 0.97          & \textbf{0.99} & 0.53          \\
        Dolly   & TF-IDF             & 0.78          & 0.73          & 0.83          & \textbf{1.00} & 0.57          \\
        Dolly   & Language Features  & 0.23          & 0.04          & 0.00          & \textbf{1.00} & 0.13          \\
        BLOOMz  & DistilBERT         & 0.01          & 0.03          & 0.03          & 0.02          & \textbf{0.99} \\
        BLOOMz  & TF-IDF             & 0.18          & 0.25          & 0.36          & 0.24          & \textbf{1.00} \\
        BLOOMz  & Language Features  & 0.00          & 0.16          & 0.08          & 0.10          & \textbf{1.00} \\
        \midrule
        ChatGPT & Statistical Tandem & \textbf{1.00} & 0.77          & 0.75          & 0.88          & 0.16          \\
        Davinci & Statistical Tandem & 0.98          & \textbf{1.00} & 0.94          & 0.92          & 0.39          \\
        Cohere  & Statistical Tandem & 0.86          & 0.79          & \textbf{1.00} & 0.65          & 0.39          \\
        Dolly   & Statistical Tandem & 0.79          & 0.74          & 0.83          & \textbf{1.00} & 0.65          \\
        BLOOMz  & Statistical Tandem & 0.18          & 0.34          & 0.39          & 0.30          & \textbf{1.00} \\
        \bottomrule
        \vspace{0.1cm}
    \end{tabular}
    \caption{
        Generalization metrics for each single-generator classifier.
        The reported value is equal to the ratio of correctly flagged documents for a generator, over all documents produced by that generator.
        In other words, a measure of recall is shown for each detector-generator combination.
        Bolded values refer to instances where the detector is trained on the productions of the generator.
        Statistical tandem indicates a combination of the Features and TF-IDF models, where a correct classification by either is counted as a success.
    }
    \label{tab:generalization}
\end{table}

Unsurprisingly, self-detection performance (bolded values) is the best in every case, showing that models are most effective at detecting text generated by the model they were trained on.
On the contrary, cross-model generalization is inconsistent, but there is much more differentiation between models in this case.
For example, ChatGPT detectors generalize moderately well to other models, especially when using the fine-tuning strategy (e.g., 0.68 on Cohere and 0.74 on Davinci).
On the other end of the spectrum, Dolly and BLOOMz detectors perform significantly worse when detecting texts from other generators.
For example, the BLOOMz detector using DistilBERT has almost negligible recall on ChatGPT (0.01) and Cohere (0.03), while Dolly-trained models using language features have very low recall scores on all other generators (e.g., 0.23 for ChatGPT).
In general, the BLOOMz targeted models barely generalize at all, but this is understandable given the low amount of training data.
But more importantly, other models also generalize very poorly to BLOOMz generations, with the best recall being 0.53 (excluding, for now, the tandem models), achieved by Dolly on TF-IDF.
Considering that the test set is made up by around 9\% of BLOOMz models, and that the first attempt at the ensemble strategy did not have BLOOMz-optimized detectors, it is easy to see how that segment of the test set could have ended up mostly misclassified in the first iteration.
Even though the BLOOMz detectors generalize poorly, they still clearly capture something about the generations by their target model (and, hopefully, the class of generators to which BLOOMz belongs in general) that all other classifiers fail to.

DistilBERT models distinguish themselves as the best-generalizing models across the board, especially those targeting ChatGPT and Davinci.
One possible explanation might be that these OpenAI models are either the largest or the most general-purpose models, thus their properties extend father away, whereas other models like Dolly describe themselves as instruction-tuned, thus making their characteristics potentially less general.
In addition, the Davinci and ChatGPT-based detectors share an interesting asymmetrical relationship, where Davinci detectors generalize very well to ChatGPT generations, but not vice versa.
This might also be explained in the context of the degree to which the model is close to the original concept of a general-purpose language model.
Davinci, in essence GPT-3, can be used for all types of completions, whereas ChatGPT is optimized as a chatbot, and takes specialized input prompts as a result.
Another standout performance is the DistilBERT classifier trained on Dolly generations, which boasts high recall on all other generators except BLOOMz.
The TF-IDF variant of the same detector loses a large chunk of recall across the board, and the feature-based classifier impresses with its ability \emph{not} to generalize.
It is important that some detectors show generalization to unknown generators, as this is after all a black-box detection task, where the test set contains a model not seen during training, namely GPT-4.
At the same time, a detector can also contribute by being excellent at detecting only its target, since it would be pointless to aim for unknown generators at the cost of dwindling prediction power over known ones.
The picture painted by Table \ref{tab:generalization} is of a good balance between the two specializations.

Since the final objective of this work with regards to the classifiers above is to use them as part of an ensemble, it would also make sense to verify how the recall changes when two models work together.
Language features and TF-IDF are orders of magnitude lighter models than DistilBERT, and this shows in their generalization performance, but this property also makes them the most aligned with the mission of developing small but efficient detectors.
For this reason, it would be interesting to verify how the two models perform when used in their intended setting, i.e. in tandem -- hence the naming of the models of the last four rows of Table \ref{tab:generalization}, "Statistical Tandem", where the two statistical models for each target generator are jointly evaluated.
If either model flags a text as positive, the text is counted as a machine-generated labelling by the joint system.
The fact that combining the models shows superior performance is, of course, a natural consequence of the operation, since both models can only benefit from the presence of the other.
What's noteworthy is the magnitude of the improvement, which is at times quite significant -- for example, Cohere over ChatGPT improves by 15 percentage points.
As a rule of thumb, models benefit most from teamwork when performance wasn't high to begin with, though there are some outliers.
In the better cases, there are improvements in the range of 10\%.
When either component strategy already displays recall of above 70\%, improvements are often minimal, but sometimes still noticeable, for example Davinci over Cohere improves by 7\% despite an already high 87\% recall achieved by the feature-based classifier.
The model that benefits the least from joint evaluation is Dolly, whose feature-based classifier seems to be almost completely dwarfed by the TF-IDF counterpart, contributing only a 1\% improvement in recall.
BLOOMz is again its own category, benefitting somewhat from the tandem experiment, but still remaining one of the lowest-scoring models.
At any rate, expectations for BLOOMz detectors were low to begin with, so any improvement in the detection of non-BLOOMz generators is a welcome surprise.

\begin{table}[ht]
    \vspace{0.1cm}
    \centering
    \begin{tabular}{lcc}
        \toprule
        Composition            & Development accuracy & Test accuracy \\

        \midrule
        TF-IDF only            & 99.527\%             & 87.179\%      \\
        Language features only & 98.777\%             & 85.466\%      \\
        DistilBERT only        & 96.250\%             & 74.790\%      \\
        TF-IDF + features      & 99.927\%             & 95.544\%      \\
        Full Model             & 99.892\%             & 97.106\%      \\
        \midrule
        Winning model          & NA                   & 96.88\%       \\
        \bottomrule
        \vspace{0.1cm}
    \end{tabular}
    \caption{
        Summary of final ensemble performance across different configurations.
        The best-performing model is the full ensemble, beating the task-winning model, whose reported accuracy is included for comparison.
    }
    \label{tab:ensemble-final}
\end{table}

\subsection{Subtask C: Shared task analysis and rankings}