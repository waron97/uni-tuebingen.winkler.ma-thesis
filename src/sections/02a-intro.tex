\section{Introduction}
\label{sec:introduction}

Recent advances in the field of language generation have unlocked the door to the widespread use of language models, across almost all fields of human life -- academics, marketing, arts, programming, and so forth.
For anyone engaging with digital media, talk of generative artificial intelligence (AI) has become a daily occurrence, and many companies are integrating language technology in some form or another.
As it stands, chatbots like ChatGPT and image generators like DALL-E are used by millions to make art, tell exciting stories, improve customer relationships, create tailor-made learning environments for students, and much more.
In this panorama of exciting new technologies, however, there is no lack of worrying signs, and potential for the misuse of these innovative tools.

Language generation has been around since last century, so its massive popularization in 2020 wasn't necessarily as groundbreaking for the discipline as it was for the general public.
What represented the biggest leap was the quality of the output that the language models were capable of.
The generation of material largely indistinguishable from human-written text was available for everyone to use through popular interfaces like ChatGPT.
Given the scale at which AI-generated content has been flooding into all manners of digital phenomena, it has become increasingly crucial to develop up-to-date technologies to detect when a piece of media (for the purposes of this work, only text is considered) is authentically human, or machine-generated.

Language generation has a huge number of positive applications, and it stands to improve people's lives in many ways. Still, the potentially nefarious uses are equally as many.
In fact, even without bad intent, it is easy for model operators to generate and use harmful language in some form.

One example of language models empowering malicious actors is phishing and scamming.
These attacks involve attempting to make their victims reveal sensitive information, or perform certain actions (for example, send money to the attacker or reveal their login information to some service).
Phishing thus far has been (fortunately) plagued by scaling issues -- to increase the number of attacks, one had to hire new human attackers, which is expensive and time-consuming.
With language generation, it could become possible to bypass this limitation by assigning some (or even most) of the work to automated systems.
As models grow more sophisticated, they might become even better suited to gain people's trust for an attacker to exploit, or trick them out of sensitive information outright.

The information landscape, already shaky in the digital age of social media, could also suffer from the malicious employment of language generation.
Disinformation campaigns in the political, social, and commercial spheres have been commonplace throughout human history, but, similar to phishing, have been notoriously expensive to scale.
The ability to generate highly convincing content to further disinforming claims is a great boon to those who seek to spread fabrications and widen divisions.
Worryingly, AI is already being shown to suit the intent to \emph{dis}inform particularly well, to the point that it may even work better as a tool for deception rather than truth.

One need not even intend to harm when employing NLG systems to inadvertently cause damage.
Plagiarism, as an example, is a very real danger with language models, since at any point they may generate one-to-one reproductions of their training data.
On the same note, if the model's training data contained biases and misinformation -- either naturally or as a result of adversarial manipulation -- the generated text could further these without obvious signs or explicit intent.
Chapter \ref{sec:threats} contains further elaboration on these issues, which were only superficially mentioned in this introduction.

In the past, automatic generation of text material relied on relatively simple technologies, and was thus comparatively easier to detect.
The ability to accurately flag generated text could be relied upon with pre-LLM systems, and the quality was often insufficient for the more sophisticated tasks.
Being alerted to the use of AI lessens its danger in most cases -- an inflammatory article is far less effective, a scam email far less convincing, and fake homework much less troublesome when the AI authorship can be reliably exposed.
Unfortunately, starting with GPT-3, not only has the quality of model output grown considerably, but even human evaluators have a tough time separating LM generations from authentic human productions.
Needless to say, automatic detection systems have also suffered a decrease in effectiveness and have had to upgrade their arsenal to keep up.

This thesis aims to delve into the field of machine-generated text detection, exploring the current state of the art, as well as limitations and future possibilities.
Outside of analyzing the various approaches to the problem, this work is meant as a contribution to the development of systems that do not forgo all size concerns in favor of performance.
Modern detection systems rely on models whose running cost is comparable to the massive language models doing the generation -- but the performance gains of this strategy do not necessarily justify the tradeoffs.

When deploying computationally heavy solutions, it is often assumed that they will not run on the end user's machine -- it is after all unreasonable to expect the average user to have hundreds of Gigabytes of RAM on hand.
Instead, the client software is only a relay to the centrally hosted service, with which it communicates over the network.
This may be acceptable in some cases, but it is not desirable in others -- for example, users may and should be reluctant to dispatch their private communications to some remote service in the name of detecting generation.

Fortunately, as with many things, system design is a game of tradeoffs with performance on the one end and complexity on the other.
Highly complex systems, relying on huge models to make predictions, will likely be the best detectors of machine-generated text.
This fact has been true in the field as well as in other areas of computational linguistics: state of the art models often take several GPUs to train and run, with high associated cost.
Still, it is often the case that 80\% of performance can be salvaged at only 20\% of the cost.
This work is written with the somewhat substantiated belief that a version of this proves true in designing solutions for machine-generated text detection as well.

There are many strategies that can be employed to detect machine generation at a reasonable computational costs.
Some examples include pretrained embeddings, which are a valid substitute of contextual embeddings provided by LLMs, and linguistically motivated features, an classical representation of text information that can be computed cheaply.
Task-specific models and even language models are still useful tools.
Not all models are huge, and there is fruitful research of transformer-based models like DistilBERT, which maintain performance close to the original with far fewer parameters.
These methods are discussed in higher detail in Chapter \ref{sec:approaches}.

Target selection is of course important, but customer-facing system designers should aim for standalone solutions that would be able to run on mid-to-high-end machines, when this is feasible.
In this work, a series of options to achieve this are presented, with some of them directly experimented with in the context of Task 8 at SemEval 2023.
The proposed models to detect LM generation employ, among other strategies, GPT-2-based perplexity metrics, linguistically motivated features, pretrained embeddings, contextual embeddings with small models, and task-specific character-level models.
A full discussion of the methods is presented in Chapter \ref{sec:task}.

Note to professor: I plan to write 2-3 more paragraphs here to conclude the introduction more elegantly, but I haven't been able to come up with a version I like yet.
